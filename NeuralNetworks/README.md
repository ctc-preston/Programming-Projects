About: This is a library of general neural network structures that are trained through either evolution (generating slightly different copies of the same neural network and selecting the models that perform the best as the next generation) or backpropagation (a method of finding the optimal changes to make to the neural network using calculus) and can easily be implemented in other projects to simulate decision making and learning. 

How to use: The neural networks are objects, to use them you must create a neural network object and pass in parameters in the order of input size, output size, hidden layer node size.... The last parameter takes in any amount of more parameters by using java's var args. An example is NeuralNetwork natty = new NeuralNetwork(3, 3, 10, 10); This will create a nueral network that takes 3 inputs, gives 3 outputs, and have 2 hidden layers each with 10 nodes. To randomize the weights and biases used in the network you must call natty.regenerate(). The NeuralNetwork is a neuralnetwork that you use evolution to train, to create a new offspring with slightly different weights and biases you must call natty.getNextOffspring(). To run the network on a set of inputs you must run natty.runNetwork(double[] in) which returns a double array of the outputs. To get the cost function you must call natty.costFunc(double[] expected) and pass in the expected outputs. The function toDesmosString will create a text file with each line representing a weight and bias node in a format that can be copied and pasted into the graphing calculator Desmos to visually view how the neuralnetwork can actually catagorize non-linear data. This will only work when there are only 2 inputs (can only graph 2 dimensions). The NeuralNetworkBackProp works a similar way with initalizing NeuralNetworkBackProp natty = new NeuralNetworkBackProp(3, 3, 10, 10); However, this neural network is not trained through evolution but through backpropagation. There is no natty.getNextOffspring() but there is natty.backpropagate(double[] inputs, double[] expectedOutputs) which takes in the inputs and expected outputs and saves slight changes to the neural network as found by gradient descent. These changes will not effect the actual neural network until natty.flush() is run.
